{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tinygrad and CUDA device.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from sentencepiece import SentencePieceProcessor\n",
    "from tinygrad import Device, Tensor, nn\n",
    "from tinygrad.nn.state import get_state_dict, safe_load, torch_load, load_state_dict, get_parameters, safe_save\n",
    "from transformers import AutoTokenizer, AutoConfig, DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "\n",
    "from timestep.config import settings\n",
    "\n",
    "device = Device.DEFAULT\n",
    "print(f'Using Tinygrad and {device} device.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_repo_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(settings.app_dir, f\"models/{hf_repo_id}/model.safetensors\")\n",
    "tokenizer_path = os.path.join(settings.app_dir, f\"models/{hf_repo_id}/tokenizer.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bos_token_id: 1; bos_token: <s>\n",
      "eos_token_id: 2; eos_token: </s>\n",
      "unk_token_id: 0; unk_token: <unk>\n",
      "Note that tokenizer.pad_id() is not the same as hf_tokenizer.pad_token_id, i.e. -1 != 2\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", eos_token=\"<|im_end|>\")\n",
    "# hf_tokenizer = AutoTokenizer.from_pretrained(hf_repo_id, pad_token=\"</s>\")\n",
    "# hf_tokenizer = AutoTokenizer.from_pretrained(hf_repo_id, bos_token=\"<|im_start|>\", eos_token=\"<|im_end|>\")\n",
    "hf_tokenizer = AutoTokenizer.from_pretrained(hf_repo_id)\n",
    "tokenizer = SentencePieceProcessor(model_file=str(tokenizer_path))\n",
    "\n",
    "print(f\"bos_token_id: {tokenizer.bos_id()}; bos_token: {tokenizer.id_to_piece(tokenizer.bos_id())}\")\n",
    "print(f\"eos_token_id: {tokenizer.eos_id()}; eos_token: {tokenizer.id_to_piece(tokenizer.eos_id())}\")\n",
    "# print(f\"pad_token_id: {tokenizer.pad_id()}; pad_token: {tokenizer.id_to_piece(tokenizer.pad_id())}\")\n",
    "print(f\"unk_token_id: {tokenizer.unk_id()}; unk_token: {tokenizer.id_to_piece(tokenizer.unk_id())}\")\n",
    "\n",
    "assert tokenizer.bos_id() == hf_tokenizer.bos_token_id, f\"{tokenizer.bos_id()} != {hf_tokenizer.bos_token_id}\"\n",
    "assert tokenizer.eos_id() == hf_tokenizer.eos_token_id, f\"{tokenizer.eos_id()} != {hf_tokenizer.eos_token_id}\"\n",
    "# assert tokenizer.pad_id() == hf_tokenizer.pad_token_id, f\"{tokenizer.pad_id()} != {hf_tokenizer.pad_token_id}\"\n",
    "assert tokenizer.unk_id() == hf_tokenizer.unk_token_id, f\"{tokenizer.unk_id()} != {hf_tokenizer.unk_token_id}\"\n",
    "\n",
    "print(f\"Note that tokenizer.pad_id() is not the same as hf_tokenizer.pad_token_id, i.e. {tokenizer.pad_id()} != {hf_tokenizer.pad_token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "open(\"template.jinja\", \"w\").write(hf_tokenizer.chat_template)\n",
    "\n",
    "hf_tokenizer.chat_template = open(\"template.jinja\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations = []\n",
    "\n",
    "with open(\"../../data/drone_training.jsonl\") as f:\n",
    "    for line in f:\n",
    "        conversations.append(json.loads(line))\n",
    "\n",
    "conversation = conversations[0]\n",
    "\n",
    "# print('messages: ', conversation[\"messages\"])\n",
    "\n",
    "# print('parallel_tool_calls: ', conversation[\"parallel_tool_calls\"])\n",
    "\n",
    "# print('tools: ', conversation[\"tools\"])\n",
    "\n",
    "# conversation = conversation[\"messages\"][0:2]\n",
    "# conversation = conversation[\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are an intelligent AI that controls a drone. Given a command or request from the user,\n",
      "call one of your functions to complete the request. If the request cannot be completed by your available functions, call the reject_request function.\n",
      "If the request is ambiguous or unclear, reject the request.</s>\n",
      "<|user|>\n",
      "Let's get the drone in the air, how high should it go?</s>\n",
      "<|assistant|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = hf_tokenizer.apply_chat_template(\n",
    "    add_generation_prompt=True,\n",
    "    # add_special_tokens=True,\n",
    "    # conversation=conversation,\n",
    "    conversation=conversation[\"messages\"][0:2], # Skip tool messages for now\n",
    "    # return_dict=True,\n",
    "    return_tensors=False,\n",
    "    tokenize=False,\n",
    "    tools=conversation[\"tools\"],\n",
    ")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[529, 29989, 5205, 29989, 29958, 13, 3492, 526, 385, 13052, 296, 319, 29902, 393, 11761, 263, 4192, 650, 29889, 11221, 263, 1899, 470, 2009, 515, 278, 1404, 29892, 13, 4804, 697, 310, 596, 3168, 304, 4866, 278, 2009, 29889, 960, 278, 2009, 2609, 367, 8676, 491, 596, 3625, 3168, 29892, 1246, 278, 12560, 29918, 3827, 740, 29889, 13, 3644, 278, 2009, 338, 22363, 681, 470, 20871, 29892, 12560, 278, 2009, 21106, 29879, 29958, 13, 29966, 29989, 1792, 29989, 29958, 13, 12024, 29915, 29879, 679, 278, 4192, 650, 297, 278, 4799, 29892, 920, 1880, 881, 372, 748, 29973, 829, 29879, 29958, 13, 29966, 29989, 465, 22137, 29989, 29958, 13]\n"
     ]
    }
   ],
   "source": [
    "encoded_prompt = tokenizer.encode(prompt)\n",
    "print(encoded_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[529, 29989, 5205, 29989, 29958, 13, 3492, 526, 385, 13052, 296, 319, 29902, 393, 11761, 263, 4192, 650, 29889, 11221, 263, 1899, 470, 2009, 515, 278, 1404, 29892, 13, 4804, 697, 310, 596, 3168, 304, 4866, 278, 2009, 29889, 960, 278, 2009, 2609, 367, 8676, 491, 596, 3625, 3168, 29892, 1246, 278, 12560, 29918, 3827, 740, 29889, 13, 3644, 278, 2009, 338, 22363, 681, 470, 20871, 29892, 12560, 278, 2009, 29889, 2, 29871, 13, 29966, 29989, 1792, 29989, 29958, 13, 12024, 29915, 29879, 679, 278, 4192, 650, 297, 278, 4799, 29892, 920, 1880, 881, 372, 748, 29973, 2, 29871, 13, 29966, 29989, 465, 22137, 29989, 29958, 13]\n"
     ]
    }
   ],
   "source": [
    "hf_tokenized_prompt = hf_tokenizer.tokenize(prompt, add_special_tokens=False)\n",
    "hf_encoded_tokenized_prompt = hf_tokenizer.convert_tokens_to_ids(hf_tokenized_prompt)\n",
    "hf_encoded_prompt = hf_tokenizer.encode(prompt, add_special_tokens=False) # Same as doing self.convert_tokens_to_ids(self.tokenize(text))\n",
    "\n",
    "assert hf_encoded_prompt == hf_encoded_tokenized_prompt, f\"\\n{hf_encoded_prompt}\\n!=\\n{hf_encoded_tokenized_prompt}\"\n",
    "\n",
    "print(hf_encoded_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are an intelligent AI that controls a drone. Given a command or request from the user,\n",
      "call one of your functions to complete the request. If the request cannot be completed by your available functions, call the reject_request function.\n",
      "If the request is ambiguous or unclear, reject the request.</s>\n",
      "<|user|>\n",
      "Let's get the drone in the air, how high should it go?</s>\n",
      "<|assistant|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "decoded_encoded_prompt = tokenizer.decode(encoded_prompt)\n",
    "print(decoded_encoded_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are an intelligent AI that controls a drone. Given a command or request from the user,\n",
      "call one of your functions to complete the request. If the request cannot be completed by your available functions, call the reject_request function.\n",
      "If the request is ambiguous or unclear, reject the request.</s> \n",
      "<|user|>\n",
      "Let's get the drone in the air, how high should it go?</s> \n",
      "<|assistant|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# decoded_hf_encoded_prompt = hf_tokenizer.decode(hf_encoded_prompt, clean_up_tokenization_spaces=True)\n",
    "decoded_hf_encoded_prompt = hf_tokenizer.decode(hf_encoded_prompt)\n",
    "print(decoded_hf_encoded_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line_a, line_b in zip(decoded_encoded_prompt.split(\"\\n\"), decoded_hf_encoded_prompt.split(\"\\n\")):\n",
    "    line_a = line_a.strip()\n",
    "    line_b = line_b.strip()\n",
    "\n",
    "    # print(line_a)\n",
    "    # print(line_b)\n",
    "    # print()\n",
    "\n",
    "    assert len(line_a) == len(line_b), f\"{len(line_a)} != {len(line_b)}\"\n",
    "    assert line_a == line_b, f\"\\n{line_a}\\n!=\\n{line_b}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert encoded_prompt == hf_encoded_prompt, f\"\\n{encoded_prompt}\\n!=\\n{hf_encoded_prompt}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert prompt == decoded_hf_encoded_prompt, f\"\\n{prompt}\\n!=\\n{decoded_hf_encoded_prompt}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import getenv\n",
    "\n",
    "from tinygrad import Tensor, Variable, Device, GlobalCounters, nn\n",
    "from tinygrad.helpers import Context, Timing, Profiling, DEBUG, JIT, getenv, colored\n",
    "\n",
    "from notebooks.Research.tinygrad.llama import Transformer, convert_from_huggingface, fix_bf16\n",
    "\n",
    "MAX_CONTEXT = getenv(\"MAX_CONTEXT\", 4096)\n",
    "\n",
    "linear = nn.Linear\n",
    "params = {\"args\": {\"dim\": 2048, \"n_layers\": 22, \"n_heads\": 32, \"n_kv_heads\": 4, \"norm_eps\": 1e-05, \"vocab_size\": 32000, \"hidden_dim\": 5632}}\n",
    "\n",
    "model = Transformer(**params[\"args\"], linear=linear, max_context=MAX_CONTEXT, jit=bool(JIT))\n",
    "\n",
    "weights = safe_load(str(model_path))\n",
    "\n",
    "if \"model.embed_tokens.weight\" in weights:\n",
    "    weights = convert_from_huggingface(weights, model, params[\"args\"][\"n_heads\"], params[\"args\"].get(\"n_kv_heads\", params[\"args\"][\"n_heads\"]))\n",
    "\n",
    "weights = fix_bf16(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ram used:  2.20 GB, freqs_cis                                         : 100%|â–ˆ| \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded weights in 2523.92 ms, 2.20 GB loaded at 0.87 GB/s\n"
     ]
    }
   ],
   "source": [
    "with Context(BEAM=0):\n",
    "    # quantize\n",
    "    # if quantize is not None:\n",
    "    # weights = linear.quantize(weights, device)\n",
    "    # for _,v in weights.items(): v.realize()\n",
    "\n",
    "    # # shard\n",
    "    # if isinstance(device, tuple):\n",
    "    # for k,v in nn.state.get_state_dict(model).items():\n",
    "    #     if 'scale' in k: v.shard_(device, axis=None)  # from quantized\n",
    "    #     elif '.attention.' in k:\n",
    "    #     if getenv(\"SHARD_KVCACHE\") and ('.wq.' in k or '.wk.' in k or '.wv.' in k): v.shard_(device, axis=0)\n",
    "    #     else: v.shard_(device, axis=-1)\n",
    "    #     elif '.feed_forward.w1.' in k: v.shard_(device, axis=0)\n",
    "    #     elif '.feed_forward.w3.' in k: v.shard_(device, axis=0)\n",
    "    #     elif '.feed_forward.' in k: v.shard_(device, axis=-1)\n",
    "    #     elif 'tok_embeddings.weight' in k: v.shard_(device, axis=0)\n",
    "    #     elif 'output.weight' in k: v.shard_(device, axis=-1)\n",
    "    #     #elif k.endswith('.weight'): v.shard_(device, axis=-1)\n",
    "    #     #elif 'norm.' in k: v.shard_(device, axis=-1)\n",
    "    #     else: v.shard_(device, axis=None)\n",
    "    #     #print(k, v.shape, v.lazydata.axis)\n",
    "\n",
    "    # replace weights in model\n",
    "    load_state_dict(model, weights, strict=False, consume=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def greedy_until(self, prompt:str, until, max_length, temperature):\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def greedy_until(prompt:str, until, max_length, temperature:float=0.0, top_k:int=0, top_p:float=0.8, alpha_f:float=0.0, alpha_p:float=0.0):\n",
    "    # toks = [self.tokenizer.bos_id()] + self.tokenizer.encode(prompt)\n",
    "    toks = [tokenizer.bos_id()] + tokenizer.encode(prompt)\n",
    "    start_pos = 0\n",
    "\n",
    "    for i in range(max_length):\n",
    "        # probs = llama.model(Tensor([toks[start_pos:]]), start_pos, temperature).realize()\n",
    "        # probs = model(Tensor([toks[start_pos:]], device=device), start_pos, temperature).realize()\n",
    "        probs = model(Tensor([toks[start_pos:]], device=device), start_pos, temperature) # TODO: the model is selecting the highest proba token already in the sample function\n",
    "        # tokens = Tensor([toks[start_pos:]], device=device)\n",
    "        # probs = model.forward_jit(\n",
    "        #    tokens, Variable(\"start_pos\", 0, MAX_CONTEXT).bind(start_pos), temperature, top_k, top_p, alpha_f, alpha_p\n",
    "        # )\n",
    "        # probs = model.forward(tokens, Variable(\"start_pos\", 0, MAX_CONTEXT).bind(start_pos), temperature, top_k, top_p, alpha_f, alpha_p)\n",
    "        # print('type(probs): ', type(probs))\n",
    "        probs_np = probs.numpy()\n",
    "        # print('type(probs_np): ', type(probs_np))\n",
    "        # print(f\"probs_np: {probs_np}\")\n",
    "        # tok = int(np.random.choice(len(probs_np), p=probs_np))\n",
    "        # tok = int(np.random.choice(probs_np.size, p=probs_np))\n",
    "        tok = int(probs_np)\n",
    "        start_pos = len(toks)\n",
    "        toks.append(tok)\n",
    "\n",
    "        # if tok == self.tokenizer.eos_id(): break\n",
    "        if tok == tokenizer.eos_id(): break\n",
    "        # output = self.tokenizer.decode(toks)\n",
    "        output = tokenizer.decode(toks)\n",
    "        # print(f\"output {i}: {output}\")\n",
    "\n",
    "        for s in until:\n",
    "          # if output.endswith(s): return output[0:-len(s)]\n",
    "          if output.endswith(s): return output\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are an intelligent AI that controls a drone. Given a command or request from the user,\n",
      "call one of your functions to complete the request. If the request cannot be completed by your available functions, call the reject_request function.\n",
      "If the request is ambiguous or unclear, reject the request.</s>\n",
      "<|user|>\n",
      "Let's get the drone in the air, how high should it go?</s>\n",
      "<|assistant|>\n",
      "The drone should be able to fly at a height of at least 10 meters (33 feet) above the ground. This is to ensure that the drone can see and avoid obstacles, such as buildings or trees, while in flight. However, the exact height may vary depending on the specifications of the drone and the environment it is operating in.\n"
     ]
    }
   ],
   "source": [
    "output = greedy_until(prompt, until=[\"<|im_end|>\", \"</s>\"], max_length=512, temperature=0.0)\n",
    "\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
